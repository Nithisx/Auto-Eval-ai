{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66890b6c",
   "metadata": {},
   "source": [
    "# Mask R-CNN — Word Segmentation for Answer Sheets (PyTorch/Torchvision)\n",
    "\n",
    "Trains **Mask R-CNN (ResNet-50-FPN v2)** to segment **handwritten words** on scanned answer sheets and, at inference, classifies each detected word as **Left/Right of the vertical border** to support grouping by questions.\n",
    "\n",
    "**Stable on Windows / Python 3.11** (uses torchvision build, no Detectron2).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880525c7",
   "metadata": {},
   "source": [
    "## 0) Install (run in your venv PowerShell)\n",
    "```powershell\n",
    "pip uninstall -y numpy opencv-python\n",
    "pip cache purge\n",
    "pip install \"numpy==1.26.4\" \"opencv-python==4.8.1.78\" pillow matplotlib\n",
    "\n",
    "# PyTorch (choose ONE)\n",
    "# CPU-only:\n",
    "pip install \"torch>=2.2\" \"torchvision>=0.17\" --index-url https://download.pytorch.org/whl/cpu\n",
    "# OR CUDA 12.1 (if supported):\n",
    "# pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# COCO API (Windows wheel)\n",
    "pip install pycocotools-windows\n",
    "# (Linux/Mac: pip install pycocotools)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab6a77b",
   "metadata": {},
   "source": [
    "## 1) Dataset (COCO format)\n",
    "Expected structure:\n",
    "```\n",
    "DATA_ROOT/\n",
    "  train/\n",
    "    images/*.jpg|png\n",
    "    annotations/instances_train.json\n",
    "  val/\n",
    "    images/*.jpg|png\n",
    "    annotations/instances_val.json\n",
    "```\n",
    "- Single category: **`word`** with `category_id=1` in COCO.\n",
    "- Export via Label Studio / VIA in COCO format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f4e853",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Tuple\n",
    "\n",
    "import json, random, time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import torchvision\n",
    "from torchvision.transforms.functional import to_tensor\n",
    "\n",
    "# COCO API\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools import mask as maskUtils\n",
    "\n",
    "from torchvision.models.detection import maskrcnn_resnet50_fpn_v2\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Torch:\", torch.__version__, \"| Torchvision:\", torchvision.__version__, \"| Device:\", DEVICE)\n",
    "\n",
    "# ---- Paths (EDIT THESE) ----\n",
    "DATA_ROOT    = Path(r\"E:\\EvaluationAI\\datasets\\answer_words_coco\")\n",
    "TRAIN_JSON   = DATA_ROOT / \"train\" / \"annotations\" / \"instances_train.json\"\n",
    "VAL_JSON     = DATA_ROOT / \"val\"   / \"annotations\" / \"instances_val.json\"\n",
    "TRAIN_IMGDIR = DATA_ROOT / \"train\" / \"images\"\n",
    "VAL_IMGDIR   = DATA_ROOT / \"val\"   / \"images\"\n",
    "\n",
    "OUT_DIR = Path(\"maskrcnn_wordseg_outputs\"); OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---- Hyperparams ----\n",
    "NUM_CLASSES = 2          # background + word\n",
    "EPOCHS      = 30\n",
    "BATCH_SIZE  = 2\n",
    "LR          = 0.005\n",
    "WORKERS     = 2\n",
    "SCORE_THR   = 0.50\n",
    "\n",
    "print(\"Train JSON:\", TRAIN_JSON.exists(), \"| Val JSON:\", VAL_JSON.exists())\n",
    "print(\"Train imgs:\", TRAIN_IMGDIR.exists(), \"| Val imgs:\", VAL_IMGDIR.exists())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3de4ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coco_poly_to_mask(segmentation, height, width):\n",
    "    # segmentation can be RLE or list of polygons\n",
    "    if isinstance(segmentation, list):\n",
    "        rles = maskUtils.frPyObjects(segmentation, height, width)\n",
    "        rle = maskUtils.merge(rles)\n",
    "    elif isinstance(segmentation['counts'], list):\n",
    "        rle = maskUtils.frPyObjects(segmentation, height, width)\n",
    "    else:\n",
    "        rle = segmentation\n",
    "    m = maskUtils.decode(rle)\n",
    "    if m.ndim == 3:\n",
    "        m = np.any(m, axis=2)\n",
    "    return m.astype(np.uint8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a930d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoWordDataset(Dataset):\n",
    "    def __init__(self, img_dir: Path, ann_json: Path):\n",
    "        self.img_dir = Path(img_dir)\n",
    "        self.coco = COCO(str(ann_json))\n",
    "        self.img_ids = list(self.coco.imgs.keys())\n",
    "        # Map category ids to contiguous [1..N]\n",
    "        self.cat_id_to_contig = {cat_id: i+1 for i, cat_id in enumerate(sorted(self.coco.getCatIds()))}\n",
    "\n",
    "    def __len__(self): return len(self.img_ids)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        img_id = self.img_ids[idx]\n",
    "        info = self.coco.loadImgs([img_id])[0]\n",
    "        img_path = self.img_dir / info[\"file_name\"]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        W, H = image.size\n",
    "\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=[img_id], iscrowd=None)\n",
    "        anns = self.coco.loadAnns(ann_ids)\n",
    "\n",
    "        masks, boxes, labels, areas, iscrowd = [], [], [], [], []\n",
    "        for ann in anns:\n",
    "            if \"segmentation\" not in ann: \n",
    "                continue\n",
    "            m = coco_poly_to_mask(ann[\"segmentation\"], H, W)\n",
    "            if m.sum() <= 0: \n",
    "                continue\n",
    "            ys, xs = np.where(m > 0)\n",
    "            x0, y0, x1, y1 = xs.min(), ys.min(), xs.max(), ys.max()\n",
    "            if (x1-x0) <= 0 or (y1-y0) <= 0: \n",
    "                continue\n",
    "            masks.append(m)\n",
    "            boxes.append([x0, y0, x1, y1])\n",
    "            labels.append(self.cat_id_to_contig.get(ann[\"category_id\"], 1))\n",
    "            areas.append(float((x1-x0)*(y1-y0)))\n",
    "            iscrowd.append(ann.get(\"iscrowd\", 0))\n",
    "\n",
    "        if len(masks)==0:\n",
    "            masks = np.zeros((0, H, W), dtype=np.uint8)\n",
    "            boxes = np.zeros((0, 4), dtype=np.float32)\n",
    "            labels= np.zeros((0,), dtype=np.int64)\n",
    "            areas = np.zeros((0,), dtype=np.float32)\n",
    "            iscrowd = np.zeros((0,), dtype=np.int64)\n",
    "        else:\n",
    "            masks = np.stack(masks, axis=0).astype(np.uint8)\n",
    "            boxes = np.array(boxes, dtype=np.float32)\n",
    "            labels= np.array(labels, dtype=np.int64)\n",
    "            areas = np.array(areas, dtype=np.float32)\n",
    "            iscrowd = np.array(iscrowd, dtype=np.int64)\n",
    "\n",
    "        image_tensor = to_tensor(image)  # [0,1]\n",
    "        target = {\n",
    "            \"boxes\": torch.as_tensor(boxes, dtype=torch.float32),\n",
    "            \"labels\": torch.as_tensor(labels, dtype=torch.int64),\n",
    "            \"masks\": torch.as_tensor(masks, dtype=torch.uint8),\n",
    "            \"image_id\": torch.as_tensor([img_id]),\n",
    "            \"area\": torch.as_tensor(areas, dtype=torch.float32),\n",
    "            \"iscrowd\": torch.as_tensor(iscrowd, dtype=torch.int64),\n",
    "        }\n",
    "        return image_tensor, target\n",
    "\n",
    "def collate_fn(batch): return tuple(zip(*batch))\n",
    "\n",
    "train_ds = CocoWordDataset(TRAIN_IMGDIR, TRAIN_JSON)\n",
    "val_ds   = CocoWordDataset(VAL_IMGDIR,   VAL_JSON)\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=WORKERS, collate_fn=collate_fn)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=1,         shuffle=False, num_workers=WORKERS, collate_fn=collate_fn)\n",
    "\n",
    "print(\"Train images:\", len(train_ds), \"| Val images:\", len(val_ds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f612fc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(num_classes: int):\n",
    "    model = maskrcnn_resnet50_fpn_v2(weights=\"DEFAULT\")\n",
    "    # Replace heads\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, 256, num_classes)\n",
    "    return model\n",
    "\n",
    "model = get_model(NUM_CLASSES).to(DEVICE)\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=LR, momentum=0.9, weight_decay=1e-4)\n",
    "lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[int(EPOCHS*0.6), int(EPOCHS*0.85)], gamma=0.1)\n",
    "\n",
    "print(\"Model ready. Trainable params:\", sum(p.numel() for p in params))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d01724",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, loader, device, epoch):\n",
    "    model.train()\n",
    "    loss_sum = 0.0; t0 = time.time()\n",
    "    for i, (images, targets) in enumerate(loader):\n",
    "        images  = [img.to(device) for img in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        loss_dict = model(images, targets)\n",
    "        loss = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_sum += float(loss.item())\n",
    "        if (i+1) % 10 == 0:\n",
    "            print(f\"[Epoch {epoch}] {i+1}/{len(loader)} loss={loss.item():.4f} \"\n",
    "                  f\"cls={loss_dict['loss_classifier']:.3f} box={loss_dict['loss_box_reg']:.3f} \"\n",
    "                  f\"mask={loss_dict['loss_mask']:.3f}\")\n",
    "    return loss_sum / max(1, len(loader))\n",
    "\n",
    "best = 1e9\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    avg = train_one_epoch(model, optimizer, train_loader, DEVICE, epoch)\n",
    "    lr_scheduler.step()\n",
    "    print(f\"Epoch {epoch} done | avg loss {avg:.4f}\")\n",
    "    torch.save(model.state_dict(), OUT_DIR / f\"maskrcnn_words_e{epoch}.pth\")\n",
    "    if avg < best:\n",
    "        best = avg\n",
    "        torch.save(model.state_dict(), OUT_DIR / \"maskrcnn_words_best.pth\")\n",
    "        print(\"  ↳ saved best checkpoint\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e27cb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_vertical_border(img_bgr: np.ndarray) -> int | None:\n",
    "    H,W = img_bgr.shape[:2]\n",
    "    gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)\n",
    "    bw = cv2.adaptiveThreshold(gray,255,cv2.ADAPTIVE_THRESH_MEAN_C,cv2.THRESH_BINARY_INV,35,10)\n",
    "    vk = cv2.getStructuringElement(cv2.MORPH_RECT,(1,max(25,H//60)))\n",
    "    vert = cv2.morphologyEx(bw, cv2.MORPH_OPEN, vk, 1)\n",
    "    edges = cv2.Canny(vert, 50, 150)\n",
    "    lines = cv2.HoughLinesP(edges, 1, np.pi/180, 120, minLineLength=int(0.5*H), maxLineGap=10)\n",
    "    if lines is None: return None\n",
    "    cand = []\n",
    "    for x1,y1,x2,y2 in lines[:,0]:\n",
    "        if abs(x1-x2) <= 6:\n",
    "            x = int((x1+x2)//2)\n",
    "            if 0 <= x <= int(0.6*W): cand.append((abs(y2-y1), x))\n",
    "    if not cand: return None\n",
    "    top = max(c[0] for c in cand)\n",
    "    left = min([c for c in cand if c[0] >= 0.8*top], key=lambda t: t[1])\n",
    "    return int(left[1])\n",
    "\n",
    "def run_inference(model, img_path: str|Path, out_dir: Path, score_thr=SCORE_THR, border_x: int|None=None):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        bgr = cv2.imread(str(img_path))\n",
    "        rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "        x = to_tensor(Image.fromarray(rgb)).to(DEVICE)\n",
    "        pred = model([x])[0]\n",
    "\n",
    "    H,W = bgr.shape[:2]\n",
    "    scores = pred[\"scores\"].detach().cpu().numpy()\n",
    "    boxes  = pred[\"boxes\"].detach().cpu().numpy().astype(np.int32)\n",
    "    masks  = pred[\"masks\"].detach().cpu().numpy()[:,0]\n",
    "\n",
    "    keep = np.where(scores >= score_thr)[0]\n",
    "    boxes, masks, scores = boxes[keep], masks[keep], scores[keep]\n",
    "\n",
    "    if border_x is None:\n",
    "        bx = detect_vertical_border(bgr)\n",
    "        border_x = bx if bx is not None else int(0.18*W)\n",
    "\n",
    "    vis = bgr.copy()\n",
    "    cv2.line(vis, (border_x,0), (border_x,H-1), (0,165,255), 2, cv2.LINE_AA)\n",
    "\n",
    "    left_dir  = out_dir / \"words_left\"\n",
    "    right_dir = out_dir / \"words_right\"\n",
    "    left_dir.mkdir(parents=True, exist_ok=True)\n",
    "    right_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for i,(box,mask,score) in enumerate(zip(boxes,masks,scores), start=1):\n",
    "        x0,y0,x1,y1 = box.tolist()\n",
    "        cx = int((x0 + x1)/2)\n",
    "        side = \"left\" if cx <= border_x else \"right\"\n",
    "        color = (0,200,0) if side==\"right\" else (255,128,0)\n",
    "\n",
    "        cv2.rectangle(vis,(x0,y0),(x1,y1),color,2,cv2.LINE_AA)\n",
    "        cv2.putText(vis, f\"{score:.2f}/{side}\", (x0, max(0,y0-6)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1, cv2.LINE_AA)\n",
    "\n",
    "        pad=2\n",
    "        xs0,ys0 = max(0,x0-pad), max(0,y0-pad)\n",
    "        xs1,ys1 = min(W-1,x1+pad), min(H-1,y1+pad)\n",
    "        crop = bgr[ys0:ys1, xs0:xs1]\n",
    "        cv2.imwrite(str((left_dir if side=='left' else right_dir)/f\"word_{i:04d}.png\"), crop)\n",
    "\n",
    "    cv2.imwrite(str(out_dir/\"overlay.png\"), vis)\n",
    "    print(f\"Saved overlay + crops to: {out_dir.resolve()} | kept {len(keep)} words (thr={score_thr})\")\n",
    "\n",
    "# Example (uncomment after training):\n",
    "# test_img = r\"E:\\EvaluationAI\\Dataset\\42.jpg\"\n",
    "# run_inference(model, test_img, OUT_DIR, score_thr=0.5)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
