{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01a81ca8",
   "metadata": {},
   "source": [
    "\n",
    "# Answer Sheet — **Segment → Group by Question → OCR per Question** (RapidOCR, CPU)\n",
    "\n",
    "This notebook is self‑contained and uses **RapidOCR (onnxruntime)** only (no Paddle, Tesseract, or EasyOCR).\n",
    "It will:\n",
    "1. Load the page **respecting EXIF orientation** (no rotation surprises).\n",
    "2. Detect **horizontal separator lines** by row‑projection + morphology.\n",
    "3. Detect **left‑margin question numbers** only in a left strip (e.g., `3)`, `4.`, `Q5`).\n",
    "4. Start a group at each question number and **end the group at the next horizontal line** (if any).\n",
    "5. OCR each group with RapidOCR and save **PNG + TXT** files and a combined **JSON**.\n",
    "\n",
    "Outputs:\n",
    "- `overlay_debug.png` — shows lines (cyan), anchors (blue), and grouping bands.\n",
    "- `groups/Q003/`, `Q004/`, … — each contains `region.png` and `text.txt`.\n",
    "- `questions.json` — all text + metadata.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedf28a1",
   "metadata": {},
   "source": [
    "\n",
    "## Install locally\n",
    "```bash\n",
    "pip install rapidocr-onnxruntime onnxruntime opencv-python numpy pillow\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da10fcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install rapidocr-onnxruntime onnxruntime opencv-python numpy pillow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b11da7fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT_PATH: E:\\EvaluationAI\\Dataset\\29.jpg\n",
      "OUTPUT_DIR: E:\\EvaluationAI\\autoevalaioutputs7\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# ==== Paths (update these) ====\n",
    "INPUT_PATH  = Path(r\"E:\\EvaluationAI\\Dataset\\29.jpg\")     # <- your page image\n",
    "OUTPUT_DIR  = Path(r\"E:\\EvaluationAI\\autoevalaioutputs7\")  # <- results will go here\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ==== Segmentation params ====\n",
    "SMOOTH_K_PCT       = 0.005   # projection smoothing kernel ≈ 0.5% of height\n",
    "PEAK_STD_MUL       = 2.0     # peaks above mean + N*std => ruling line\n",
    "MIN_LINE_GAP_PCT   = 0.06    # min vertical spacing between lines (6% height)\n",
    "MAX_THICK_PCT      = 0.05    # max thickness for a line (5% of height)\n",
    "PAD_INSIDE_PX      = 18      # padding inside segment bounds\n",
    "\n",
    "# ==== Left-strip OCR for anchors ====\n",
    "LEFT_STRIP_RATIO   = 0.32    # scan up to 32% of width on the left\n",
    "ANCHOR_CONF_MIN    = 0.30    # min confidence to keep a candidate\n",
    "LEFT_PAD_BORDER_PX = 16      # draw border slightly to right of numbers\n",
    "# Question number patterns to accept:\n",
    "ANCHOR_REGEXES     = [\n",
    "    r\"^\\(?\\d{1,2}\\)?$\",        # 3  (3)\n",
    "    r\"^\\(?\\d{1,2}\\)?[.)]$\",    # 3)  3.\n",
    "    r\"^[Qq]\\s*\\d{1,2}[.)]?$\",  # Q3  q12.\n",
    "]\n",
    "\n",
    "# ==== OCR / compose params ====\n",
    "CONF_MIN           = 0.30    # drop very low-confidence tokens\n",
    "ROW_GAP_FACTOR     = 0.7     # row clustering threshold = median_height * factor\n",
    "\n",
    "print(\"INPUT_PATH:\", INPUT_PATH)\n",
    "print(\"OUTPUT_DIR:\", OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f6e8b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re, json\n",
    "from typing import List, Tuple, Dict, Any\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "def read_image_exif_bgr(path: Path):\n",
    "    im = Image.open(path)\n",
    "    im = ImageOps.exif_transpose(im).convert(\"RGB\")\n",
    "    arr = np.array(im)\n",
    "    return cv2.cvtColor(arr, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "def save_image(img_bgr, name: str) -> Path:\n",
    "    p = OUTPUT_DIR / name\n",
    "    p.parent.mkdir(parents=True, exist_ok=True)\n",
    "    cv2.imwrite(str(p), img_bgr)\n",
    "    return p\n",
    "\n",
    "def save_text(txt: str, name: str) -> Path:\n",
    "    p = OUTPUT_DIR / name\n",
    "    p.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(p, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write((txt or \"\").strip() + \"\\n\")\n",
    "    return p\n",
    "\n",
    "def save_json(obj: Any, name: str) -> Path:\n",
    "    p = OUTPUT_DIR / name\n",
    "    p.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(p, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, indent=2, ensure_ascii=False)\n",
    "    return p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe607e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def detect_horizontal_lines(img_bgr: np.ndarray,\n",
    "                            smooth_k_pct=0.005,\n",
    "                            peak_std_mul=2.0,\n",
    "                            min_line_gap_pct=0.06,\n",
    "                            max_thick_pct=0.05) -> List[int]:\n",
    "    H, W = img_bgr.shape[:2]\n",
    "    gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)\n",
    "    g = cv2.GaussianBlur(gray, (5,5), 0)\n",
    "\n",
    "    # Row ink projection (text/lines darker => more 'ink')\n",
    "    ink = 255 - g\n",
    "    row_sum = ink.sum(axis=1).astype(np.float32)\n",
    "\n",
    "    k = max(11, int(H * smooth_k_pct) | 1)  # odd kernel length\n",
    "    kernel = np.ones(k, dtype=np.float32) / k\n",
    "    smooth = np.convolve(row_sum, kernel, mode=\"same\")\n",
    "\n",
    "    mu, sd = float(smooth.mean()), float(smooth.std())\n",
    "    thr = mu + peak_std_mul*sd\n",
    "    mask = smooth > thr\n",
    "\n",
    "    lines_raw = []\n",
    "    i = 0\n",
    "    while i < H:\n",
    "        if mask[i]:\n",
    "            j = i\n",
    "            while j+1 < H and mask[j+1]:\n",
    "                j += 1\n",
    "            idx = np.arange(i, j+1)\n",
    "            center = int(np.average(idx, weights=smooth[i:j+1]))\n",
    "            thickness = j - i + 1\n",
    "            strength  = float(smooth[i:j+1].max())\n",
    "            lines_raw.append((center, thickness, strength))\n",
    "            i = j + 1\n",
    "        else:\n",
    "            i += 1\n",
    "\n",
    "    # keep thin peaks\n",
    "    lines_raw = [l for l in lines_raw if l[1] <= max_thick_pct*H]\n",
    "\n",
    "    # pick strongest, spaced apart\n",
    "    lines_raw.sort(key=lambda t: t[2], reverse=True)\n",
    "    picked = []\n",
    "    min_gap = int(min_line_gap_pct * H)\n",
    "    for c,t,s in lines_raw:\n",
    "        if not picked or all(abs(c - pc) >= min_gap for pc,_t,_s in picked):\n",
    "            picked.append((c,t,s))\n",
    "    centers = sorted([c for c,_t,_s in picked])\n",
    "    return centers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e437535",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from rapidocr_onnxruntime import RapidOCR\n",
    "\n",
    "_ocr = None\n",
    "def get_ocr():\n",
    "    global _ocr\n",
    "    if _ocr is None:\n",
    "        _ocr = RapidOCR(det_use_cuda=False, rec_use_cuda=False)\n",
    "    return _ocr\n",
    "\n",
    "def rapidocr(img_bgr: np.ndarray) -> list:\n",
    "    ocr = get_ocr()\n",
    "    result, _ = ocr(img_bgr)  # list of [box(4x2), text, score]\n",
    "    return result or []\n",
    "\n",
    "def compose_text_from_rapidocr(results: list,\n",
    "                               conf_min=0.30,\n",
    "                               row_gap_factor=0.7) -> str:\n",
    "    if not results: return \"\"\n",
    "    items = []\n",
    "    for box, text, score in results:\n",
    "        if not text or float(score) < conf_min:\n",
    "            continue\n",
    "        b = np.array(box, dtype=np.float32)\n",
    "        xs = b[:,0]; ys = b[:,1]\n",
    "        x0,y0,x1,y1 = int(xs.min()), int(ys.min()), int(xs.max()), int(ys.max())\n",
    "        cx, cy = (x0+x1)/2.0, (y0+y1)/2.0\n",
    "        items.append({\"text\": text.strip(), \"conf\": float(score),\n",
    "                      \"bbox\": (x0,y0,x1,y1), \"cx\": cx, \"cy\": cy, \"h\": (y1-y0)})\n",
    "    if not items: return \"\"\n",
    "\n",
    "    hs = np.array([it[\"h\"] for it in items], dtype=np.float32)\n",
    "    row_thr = max(12.0, float(np.median(hs) * row_gap_factor))\n",
    "\n",
    "    items.sort(key=lambda it: it[\"cy\"])\n",
    "    rows = []\n",
    "    cur = [items[0]]\n",
    "    for it in items[1:]:\n",
    "        if abs(it[\"cy\"] - cur[-1][\"cy\"]) <= row_thr:\n",
    "            cur.append(it)\n",
    "        else:\n",
    "            rows.append(cur); cur = [it]\n",
    "    rows.append(cur)\n",
    "\n",
    "    lines = []\n",
    "    for row in rows:\n",
    "        row.sort(key=lambda it: it[\"cx\"])\n",
    "        parts = [it[\"text\"] for it in row if it[\"text\"]]\n",
    "        if parts:\n",
    "            lines.append(\" \".join(parts))\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def preprocess_block(block_bgr: np.ndarray, mode: int = 1) -> np.ndarray:\n",
    "    g = cv2.cvtColor(block_bgr, cv2.COLOR_BGR2GRAY)\n",
    "    if mode == 1:\n",
    "        clahe = cv2.createCLAHE(2.0, (8,8))\n",
    "        g = clahe.apply(g)\n",
    "        th = cv2.adaptiveThreshold(g, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "                                   cv2.THRESH_BINARY, 35, 10)\n",
    "    else:\n",
    "        g = cv2.GaussianBlur(g, (3,3), 0)\n",
    "        _, th = cv2.threshold(g, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "    return cv2.cvtColor(th, cv2.COLOR_GRAY2BGR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71a70873",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def is_anchor_text(text: str) -> bool:\n",
    "    t = text.strip()\n",
    "    return any(re.match(p, t) for p in ANCHOR_REGEXES)\n",
    "\n",
    "def detect_anchors_left_strip(img_bgr: np.ndarray,\n",
    "                              xr: float,\n",
    "                              conf_min: float = 0.30) -> List[Dict[str, Any]]:\n",
    "    H, W = img_bgr.shape[:2]\n",
    "    x_max = int(max(1, min(W-1, xr * W)))\n",
    "    crop = img_bgr[:, :x_max].copy()\n",
    "    res = rapidocr(crop)\n",
    "    anchors = []\n",
    "    for box, text, score in res:\n",
    "        if float(score) < max(0.0, conf_min): continue\n",
    "        if not is_anchor_text(text): continue\n",
    "        b = np.array(box, dtype=np.float32)\n",
    "        xs = b[:,0]; ys = b[:,1]\n",
    "        x0,y0,x1,y1 = int(xs.min()), int(ys.min()), int(xs.max()), int(ys.max())\n",
    "        # adjust to absolute x\n",
    "        anchors.append({\"text\": text, \"conf\": float(score),\n",
    "                        \"bbox\": [x0, y0, x1, y1],\n",
    "                        \"center_y\": float((y0+y1)/2.0),\n",
    "                        \"right_x\": int(x1)})\n",
    "    # sort by Y & dedupe close ones\n",
    "    anchors.sort(key=lambda a: a[\"center_y\"])\n",
    "    dedup = []\n",
    "    for a in anchors:\n",
    "        if dedup and abs(a[\"center_y\"] - dedup[-1][\"center_y\"]) < 18:\n",
    "            # keep the larger/confident\n",
    "            prev = dedup[-1]\n",
    "            pa = (prev[\"bbox\"][2]-prev[\"bbox\"][0])*(prev[\"bbox\"][3]-prev[\"bbox\"][1])\n",
    "            ca = (a[\"bbox\"][2]-a[\"bbox\"][0])*(a[\"bbox\"][3]-a[\"bbox\"][1])\n",
    "            if ca > pa:\n",
    "                dedup[-1] = a\n",
    "        else:\n",
    "            dedup.append(a)\n",
    "    return dedup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b3df326",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def segment_blocks(img_bgr: np.ndarray) -> Dict[str, Any]:\n",
    "    H, W = img_bgr.shape[:2]\n",
    "\n",
    "    # 1) detect lines & anchors\n",
    "    line_ys = detect_horizontal_lines(img_bgr,\n",
    "                                      smooth_k_pct=SMOOTH_K_PCT,\n",
    "                                      peak_std_mul=PEAK_STD_MUL,\n",
    "                                      min_line_gap_pct=MIN_LINE_GAP_PCT,\n",
    "                                      max_thick_pct=MAX_THICK_PCT)\n",
    "    anchors = detect_anchors_left_strip(img_bgr, LEFT_STRIP_RATIO, conf_min=ANCHOR_CONF_MIN)\n",
    "\n",
    "    # 2) border just to the right of left numbers (95th percentile)\n",
    "    if anchors:\n",
    "        right_edges = [a[\"right_x\"] for a in anchors]\n",
    "        x_border = int(np.percentile(np.array(right_edges), 95) + LEFT_PAD_BORDER_PX)\n",
    "        x_border = min(max(x_border, int(0.08*W)), int(0.5*W))\n",
    "    else:\n",
    "        # no anchors found — use a conservative 28% width\n",
    "        x_border = int(0.28 * W)\n",
    "\n",
    "    # 3) compute vertical bands per question:\n",
    "    #    start at anchor y, end at the earliest of (next separator below, mid-to-next-anchor, bottom)\n",
    "    groups = []\n",
    "    if anchors:\n",
    "        for i, anc in enumerate(anchors):\n",
    "            a_y = int(anc[\"center_y\"])\n",
    "            next_a_y = int(anchors[i+1][\"center_y\"]) if i+1 < len(anchors) else H-1\n",
    "            # first separator below current anchor\n",
    "            sep_candidates = [y for y in line_ys if y > a_y + 8]\n",
    "            sep_y = min(sep_candidates) if sep_candidates else None\n",
    "            mid_to_next = int((a_y + next_a_y)/2)\n",
    "\n",
    "            y_start = 0 if i == 0 else int((anchors[i-1][\"center_y\"] + a_y)/2)\n",
    "            y_start = max(0, y_start + PAD_INSIDE_PX)\n",
    "\n",
    "            y_end = min([v for v in [sep_y, mid_to_next, H-1] if v is not None])\n",
    "            y_end = max(y_start+1, y_end - PAD_INSIDE_PX)\n",
    "\n",
    "            groups.append({\"qid\": i+1,\n",
    "                           \"label\": f\"Q{re.sub('[^0-9]','', anc['text']) or i+1:>}\",  # try to use the number\n",
    "                           \"anchor_text\": anc[\"text\"],\n",
    "                           \"anchor_conf\": anc[\"conf\"],\n",
    "                           \"y0\": int(y_start), \"y1\": int(y_end)})\n",
    "    else:\n",
    "        # fallback: split by lines only\n",
    "        edges = [0] + line_ys + [H-1]\n",
    "        for i in range(len(edges)-1):\n",
    "            y0 = max(0, edges[i] + (PAD_INSIDE_PX if i>0 else 0))\n",
    "            y1 = min(H-1, edges[i+1] - PAD_INSIDE_PX)\n",
    "            if y1 <= y0: continue\n",
    "            groups.append({\"qid\": i+1, \"label\": f\"Q{i+1:02d}\",\n",
    "                           \"anchor_text\": None, \"anchor_conf\": None,\n",
    "                           \"y0\": int(y0), \"y1\": int(y1)})\n",
    "\n",
    "    return {\"lines\": line_ys, \"anchors\": anchors, \"border_x\": int(x_border), \"groups\": groups}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "71a7efce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected lines: [2848]\n",
      "Anchors: []\n",
      "Border x: 766\n",
      "Q01  y=[0, 2830]  chars=96\n",
      "Q02  y=[2866, 3997]  chars=67\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def run_pipeline():\n",
    "    img = read_image_exif_bgr(INPUT_PATH)\n",
    "    H, W = img.shape[:2]\n",
    "\n",
    "    seg = segment_blocks(img)\n",
    "    line_ys   = seg[\"lines\"]\n",
    "    anchors   = seg[\"anchors\"]\n",
    "    x_border  = seg[\"border_x\"]\n",
    "    groups    = seg[\"groups\"]\n",
    "\n",
    "    out = {\"input\": str(INPUT_PATH), \"lines\": line_ys,\n",
    "           \"anchors\": anchors, \"border_x\": x_border, \"questions\": []}\n",
    "\n",
    "    # overlay\n",
    "    overlay = img.copy()\n",
    "    # lines (cyan)\n",
    "    for y in line_ys:\n",
    "        cv2.line(overlay, (0,y), (W-1,y), (255,255,0), 2, lineType=cv2.LINE_AA)\n",
    "    # border (orange)\n",
    "    cv2.line(overlay, (x_border, 0), (x_border, H-1), (0,165,255), 2, lineType=cv2.LINE_AA)\n",
    "    # anchors (blue)\n",
    "    for a in anchors:\n",
    "        x0,y0,x1,y1 = a[\"bbox\"]\n",
    "        cv2.rectangle(overlay, (x0,y0), (x1,y1), (255,128,0), 1, lineType=cv2.LINE_AA)\n",
    "        cv2.putText(overlay, a[\"text\"], (x0, max(0,y0-5)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,128,0), 1, cv2.LINE_AA)\n",
    "    # groups (green bands)\n",
    "    for g in groups:\n",
    "        y0,y1 = g[\"y0\"], g[\"y1\"]\n",
    "        cv2.rectangle(overlay, (0,y0), (W-1,y1), (0,200,0), 1, lineType=cv2.LINE_AA)\n",
    "\n",
    "    save_image(overlay, \"overlay_debug.png\")\n",
    "\n",
    "    # OCR each group (right of border only)\n",
    "    qroot = OUTPUT_DIR / \"groups\"\n",
    "    qroot.mkdir(parents=True, exist_ok=True)\n",
    "    for g in groups:\n",
    "        y0,y1 = g[\"y0\"], g[\"y1\"]\n",
    "        # crop full width, but OCR only region to the right of border to avoid left margin\n",
    "        region = img[y0:y1, 0:W].copy()\n",
    "        region_right = img[y0:y1, x_border:W].copy()\n",
    "\n",
    "        # two-pass preprocessing (same backend)\n",
    "        r1 = preprocess_block(region_right, 1)\n",
    "        txt = compose_text_from_rapidocr(rapidocr(r1), conf_min=CONF_MIN, row_gap_factor=ROW_GAP_FACTOR)\n",
    "        if not txt.strip():\n",
    "            r2 = preprocess_block(region_right, 2)\n",
    "            txt = compose_text_from_rapidocr(rapidocr(r2), conf_min=max(0.0, CONF_MIN*0.9), row_gap_factor=ROW_GAP_FACTOR)\n",
    "\n",
    "        # save\n",
    "        label = g[\"label\"] if isinstance(g[\"label\"], str) else f\"Q{g['qid']:02d}\"\n",
    "        qdir = qroot / label\n",
    "        qdir.mkdir(parents=True, exist_ok=True)\n",
    "        save_image(region,  f\"{label}/region.png\")\n",
    "        save_text(txt,      f\"{label}/text.txt\")\n",
    "\n",
    "        out[\"questions\"].append({\n",
    "            \"label\": label, \"qid\": g[\"qid\"], \"y_range\": [int(y0), int(y1)],\n",
    "            \"image\": str((OUTPUT_DIR / f\"{label}/region.png\").as_posix()).replace(str(OUTPUT_DIR.as_posix())+\"/\",\"\"),\n",
    "            \"text_file\": str((OUTPUT_DIR / f\"{label}/text.txt\").as_posix()).replace(str(OUTPUT_DIR.as_posix())+\"/\",\"\"),\n",
    "            \"text\": txt\n",
    "        })\n",
    "\n",
    "    save_json(out, \"questions.json\")\n",
    "    return out\n",
    "\n",
    "res = run_pipeline()\n",
    "print(\"Detected lines:\", res[\"lines\"])\n",
    "print(\"Anchors:\", [(a['text'], int(a['center_y'])) for a in res['anchors']])\n",
    "print(\"Border x:\", res[\"border_x\"])\n",
    "for q in res[\"questions\"]:\n",
    "    print(f\"{q['label']}  y={q['y_range']}  chars={len(q['text'])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6ecb84",
   "metadata": {},
   "source": [
    "## Inspect outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b87c65b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overlay: E:/EvaluationAI/autoevalaioutputs7/overlay_debug.png\n",
      "Groups folder: E:/EvaluationAI/autoevalaioutputs7/groups\n",
      "\n",
      "=== Q01 ===\n",
      "e energPfon\n",
      "OS Hhee totally 1o bPls\n",
      "Pphes\n",
      "but\" H\n",
      "enqiPokon tspeifke\n",
      "02517\n",
      "Ccsingdfheco mthean\n",
      "on\n",
      "\n",
      "=== Q02 ===\n",
      "P\n",
      "Yoiteapf Akry.\n",
      "a PvPble Sy P Hhen\n",
      "d ypquy1\n",
      "PS ChPnocue Bermorindg\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pprint import pprint\n",
    "print(\"Overlay:\", (OUTPUT_DIR / \"overlay_debug.png\").as_posix())\n",
    "print(\"Groups folder:\", (OUTPUT_DIR / \"groups\").as_posix())\n",
    "with open(OUTPUT_DIR / \"questions.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "for q in data[\"questions\"]:\n",
    "    print(\"\\n===\", q[\"label\"], \"===\")\n",
    "    print(q[\"text\"][:600])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
