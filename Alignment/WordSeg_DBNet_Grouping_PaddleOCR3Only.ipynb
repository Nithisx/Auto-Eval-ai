{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba5f2140",
   "metadata": {},
   "source": [
    "# Word Segmentation + Anchor Grouping (PaddleOCR 3.x only, **no OCR**)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e4ec16",
   "metadata": {},
   "source": [
    "\n",
    "This notebook is **locked to PaddleOCR 3.x API** to avoid legacy-argument errors like `Unknown argument: det/show_log`.\n",
    "- Uses **DBNet++** detector via PaddleOCR **3.x** (PaddleX pipeline).\n",
    "- Segments **every word** (no recognition).\n",
    "- Auto- or fixed-border to collect right-side words per left anchor.\n",
    "\n",
    "If you want a 2.x–compatible version, pin: `pip install paddlepaddle==2.6.1 paddleocr==2.7.1` and use the previous notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dca9d2",
   "metadata": {},
   "source": [
    "\n",
    "## Install\n",
    "```bash\n",
    "pip install opencv-python numpy pillow\n",
    "pip install paddlepaddle paddleocr   # 3.x API\n",
    "# (GPU optional) pip install paddlepaddle-gpu  # wheel must match your CUDA\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dc6681c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT_PATH: E:\\EvaluationAI\\Dataset\\29.jpg\n",
      "OUTPUT_DIR: E:\\EvaluationAI\\autoevalaioutputs\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Paths ---\n",
    "INPUT_PATH = Path(r\"E:\\EvaluationAI\\Dataset\\29.jpg\")   # change if needed\n",
    "OUTPUT_DIR = Path(r\"E:\\EvaluationAI\\autoevalaioutputs\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Alignment (optional but recommended) ---\n",
    "ALIGN_TO_A4        = True\n",
    "A4_SIZE_PX         = (2480, 3508)  # (W, H) A4 @ 300DPI\n",
    "\n",
    "# --- Detector params (tune for **word** granularity) ---\n",
    "USE_GPU            = False\n",
    "DET_DB_BOX_THRESH  = 0.65\n",
    "DET_DB_THRESH      = 0.40\n",
    "DET_DB_UNCLIP      = 1.40\n",
    "MIN_AREA_PX        = 30             # drop tiny boxes\n",
    "\n",
    "# --- Border / anchors ---\n",
    "BORDER_MODE        = \"auto\"         # \"auto\" or \"fixed\"\n",
    "FIXED_BORDER_RATIO = 0.30           # used when BORDER_MODE == \"fixed\"\n",
    "LEFT_CAND_MAX_XR   = 0.50           # anchor cand: center-x <= 50% of width\n",
    "LEFT_PAD_PX        = 20             # push border slightly right of anchors\n",
    "\n",
    "# --- Grouping pads ---\n",
    "TOP_PAD            = 6\n",
    "BOTTOM_PAD         = 6\n",
    "\n",
    "print(\"INPUT_PATH:\", INPUT_PATH)\n",
    "print(\"OUTPUT_DIR:\", OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "068c008c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, json, math\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def ensure_img(path: Path) -> np.ndarray:\n",
    "    img = cv2.imread(str(path))\n",
    "    if img is None:\n",
    "        raise FileNotFoundError(f\"Could not load image: {path}\")\n",
    "    return img\n",
    "\n",
    "def save_image(img_bgr: np.ndarray, name: str) -> Path:\n",
    "    p = OUTPUT_DIR / name\n",
    "    p.parent.mkdir(parents=True, exist_ok=True)\n",
    "    cv2.imwrite(str(p), img_bgr)\n",
    "    return p\n",
    "\n",
    "def save_json(obj: Any, name: str) -> Path:\n",
    "    p = OUTPUT_DIR / name\n",
    "    p.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(p, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, indent=2, ensure_ascii=False)\n",
    "    return p\n",
    "\n",
    "def _find_contours(*args, **kwargs):\n",
    "    res = cv2.findContours(*args, **kwargs)\n",
    "    return res[-2], res[-1]\n",
    "\n",
    "def detect_page_quad(gray: np.ndarray) -> Optional[np.ndarray]:\n",
    "    g = cv2.GaussianBlur(gray, (5,5), 0)\n",
    "    edges = cv2.Canny(g, 50, 150)\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5,5))\n",
    "    edges = cv2.dilate(edges, kernel, iterations=1)\n",
    "    edges = cv2.morphologyEx(edges, cv2.MORPH_CLOSE, kernel, iterations=1)\n",
    "    cnts, _ = _find_contours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if not cnts: return None\n",
    "    cnt = max(cnts, key=cv2.contourArea)\n",
    "    peri = cv2.arcLength(cnt, True)\n",
    "    approx = cv2.approxPolyDP(cnt, 0.02 * peri, True)\n",
    "    if len(approx) < 4:\n",
    "        approx = cv2.convexHull(cnt)\n",
    "    approx = approx.reshape(-1, 2).astype(np.float32)\n",
    "    if len(approx) != 4:\n",
    "        rect = cv2.minAreaRect(cnt)\n",
    "        box  = cv2.boxPoints(rect)\n",
    "        approx = box.astype(np.float32)\n",
    "    return approx\n",
    "\n",
    "def order_corners(pts: np.ndarray) -> np.ndarray:\n",
    "    s = pts.sum(axis=1)\n",
    "    d = np.diff(pts, axis=1)[:,0]\n",
    "    tl = pts[np.argmin(s)]\n",
    "    br = pts[np.argmax(s)]\n",
    "    tr = pts[np.argmin(d)]\n",
    "    bl = pts[np.argmax(d)]\n",
    "    return np.array([tl, tr, br, bl], dtype=np.float32)\n",
    "\n",
    "def align_to_a4(img_bgr: np.ndarray, out_size=(2480,3508)) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)\n",
    "    quad = detect_page_quad(gray)\n",
    "    if quad is None:\n",
    "        h, w = img_bgr.shape[:2]\n",
    "        quad = np.array([[0,0],[w-1,0],[w-1,h-1],[0,h-1]], dtype=np.float32)\n",
    "    src = order_corners(quad)\n",
    "    dst = np.array([[0,0],[out_size[0]-1,0],[out_size[0]-1,out_size[1]-1],[0,out_size[1]-1]], dtype=np.float32)\n",
    "    M = cv2.getPerspectiveTransform(src, dst)\n",
    "    warped = cv2.warpPerspective(img_bgr, M, out_size)\n",
    "    g  = cv2.cvtColor(warped, cv2.COLOR_BGR2GRAY)\n",
    "    cl = cv2.createCLAHE(2.0, (8,8))\n",
    "    g2 = cl.apply(g)\n",
    "    warped = cv2.cvtColor(g2, cv2.COLOR_GRAY2BGR)\n",
    "    return warped, src\n",
    "\n",
    "def poly_to_bbox(poly: np.ndarray) -> Tuple[int,int,int,int]:\n",
    "    xs = poly[:,0]; ys = poly[:,1]\n",
    "    x0 = int(np.floor(xs.min())); y0 = int(np.floor(ys.min()))\n",
    "    x1 = int(np.ceil(xs.max()));  y1 = int(np.ceil(ys.max()))\n",
    "    return x0,y0,x1,y1\n",
    "\n",
    "def save_crop(img: np.ndarray, bbox_xyxy, path: Path):\n",
    "    x0,y0,x1,y1 = bbox_xyxy\n",
    "    h, w = img.shape[:2]\n",
    "    x0 = max(0,x0); y0 = max(0,y0); x1 = min(w-1, x1); y1 = min(h-1, y1)\n",
    "    if x1 > x0 and y1 > y0:\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        cv2.imwrite(str(path), img[y0:y1, x0:x1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c756b4f4",
   "metadata": {},
   "source": [
    "## PaddleOCR 3.x detector wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "888d1e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from paddleocr import PaddleOCR\n",
    "from typing import List\n",
    "import numpy as np\n",
    "\n",
    "def _extract_polys_v3(result) -> List[np.ndarray]:\n",
    "    polys: List[np.ndarray] = []\n",
    "    # result is usually a list (one per page)\n",
    "    pages = result if isinstance(result, (list, tuple)) else [result]\n",
    "    for pg in pages:\n",
    "        if isinstance(pg, dict):\n",
    "            for key in (\"word_boxes\", \"boxes\", \"det\", \"text_det_results\", \"dt_boxes\", \"dt_polys\", \"polygons\"):\n",
    "                if key in pg and isinstance(pg[key], (list, tuple)):\n",
    "                    for box in pg[key]:\n",
    "                        arr = np.array(box, dtype=np.float32)\n",
    "                        if arr.ndim == 2 and arr.shape[0] >= 4:\n",
    "                            polys.append(arr)\n",
    "        elif isinstance(pg, (list, tuple)):\n",
    "            # some 3.x returns a list of polys already\n",
    "            for box in pg:\n",
    "                arr = np.array(box, dtype=np.float32)\n",
    "                if arr.ndim == 2 and arr.shape[0] >= 4:\n",
    "                    polys.append(arr)\n",
    "        elif hasattr(pg, \"shape\"):\n",
    "            arr = np.array(pg, dtype=np.float32)\n",
    "            if arr.ndim == 3 and arr.shape[1] >= 4:\n",
    "                for poly in arr:\n",
    "                    polys.append(np.array(poly, dtype=np.float32))\n",
    "    return polys\n",
    "\n",
    "def run_dbnet_detector(img_bgr):\n",
    "    ocr = PaddleOCR(\n",
    "        lang='en',\n",
    "        use_gpu=USE_GPU,\n",
    "        ocr_version='PP-OCRv4',\n",
    "        # detector thresholds:\n",
    "        text_det_box_thresh=DET_DB_BOX_THRESH,\n",
    "        text_det_thresh=DET_DB_THRESH,\n",
    "        text_det_unclip_ratio=DET_DB_UNCLIP,\n",
    "        # request word boxes; disable recognition\n",
    "        return_word_box=True,\n",
    "        text_recognition_model_name=None,\n",
    "    )\n",
    "    res = ocr.ocr(img_bgr)  # 3.x: no show_log/cls/det/rec args\n",
    "    polys = _extract_polys_v3(res)\n",
    "    return polys\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b7d2fc",
   "metadata": {},
   "source": [
    "## Run segmentation → auto/fixed border → group right-side words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1217a41",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown argument: use_gpu",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 117\u001b[39m\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m groups:\n\u001b[32m    115\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mg[\u001b[33m'\u001b[39m\u001b[33mgroup\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(g[\u001b[33m'\u001b[39m\u001b[33mwords\u001b[39m\u001b[33m'\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m right-side words; y_range=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mg[\u001b[33m'\u001b[39m\u001b[33my_range\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m \u001b[43mrun_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mrun_pipeline\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      3\u001b[39m img  = align_to_a4(img0, A4_SIZE_PX)[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m ALIGN_TO_A4 \u001b[38;5;28;01melse\u001b[39;00m img0\n\u001b[32m      5\u001b[39m H, W = img.shape[:\u001b[32m2\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m polys = \u001b[43mrun_dbnet_detector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m words = []\n\u001b[32m      9\u001b[39m crops_dir = OUTPUT_DIR / \u001b[33m\"\u001b[39m\u001b[33mcrops\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 31\u001b[39m, in \u001b[36mrun_dbnet_detector\u001b[39m\u001b[34m(img_bgr)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_dbnet_detector\u001b[39m(img_bgr):\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     ocr = \u001b[43mPaddleOCR\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlang\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43men\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_gpu\u001b[49m\u001b[43m=\u001b[49m\u001b[43mUSE_GPU\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m        \u001b[49m\u001b[43mocr_version\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mPP-OCRv4\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# detector thresholds:\u001b[39;49;00m\n\u001b[32m     36\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext_det_box_thresh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDET_DB_BOX_THRESH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext_det_thresh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDET_DB_THRESH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext_det_unclip_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDET_DB_UNCLIP\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# request word boxes; disable recognition\u001b[39;49;00m\n\u001b[32m     40\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_word_box\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext_recognition_model_name\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m     res = ocr.ocr(img_bgr)  \u001b[38;5;66;03m# 3.x: no show_log/cls/det/rec args\u001b[39;00m\n\u001b[32m     44\u001b[39m     polys = _extract_polys_v3(res)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\EvaluationAI\\env\\Lib\\site-packages\\paddleocr\\_pipelines\\ocr.py:163\u001b[39m, in \u001b[36mPaddleOCR.__init__\u001b[39m\u001b[34m(self, doc_orientation_classify_model_name, doc_orientation_classify_model_dir, doc_unwarping_model_name, doc_unwarping_model_dir, text_detection_model_name, text_detection_model_dir, textline_orientation_model_name, textline_orientation_model_dir, textline_orientation_batch_size, text_recognition_model_name, text_recognition_model_dir, text_recognition_batch_size, use_doc_orientation_classify, use_doc_unwarping, use_textline_orientation, text_det_limit_side_len, text_det_limit_type, text_det_thresh, text_det_box_thresh, text_det_unclip_ratio, text_det_input_shape, text_rec_score_thresh, return_word_box, text_rec_input_shape, lang, ocr_version, **kwargs)\u001b[39m\n\u001b[32m    159\u001b[39m         base_params[name] = val\n\u001b[32m    161\u001b[39m \u001b[38;5;28mself\u001b[39m._params = params\n\u001b[32m--> \u001b[39m\u001b[32m163\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mbase_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\EvaluationAI\\env\\Lib\\site-packages\\paddleocr\\_pipelines\\base.py:63\u001b[39m, in \u001b[36mPaddleXPipelineWrapper.__init__\u001b[39m\u001b[34m(self, paddlex_config, **common_args)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m()\n\u001b[32m     62\u001b[39m \u001b[38;5;28mself\u001b[39m._paddlex_config = paddlex_config\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m \u001b[38;5;28mself\u001b[39m._common_args = \u001b[43mparse_common_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcommon_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_enable_hpi\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_DEFAULT_ENABLE_HPI\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[38;5;28mself\u001b[39m._merged_paddlex_config = \u001b[38;5;28mself\u001b[39m._get_merged_paddlex_config()\n\u001b[32m     67\u001b[39m \u001b[38;5;28mself\u001b[39m.paddlex_pipeline = \u001b[38;5;28mself\u001b[39m._create_paddlex_pipeline()\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\EvaluationAI\\env\\Lib\\site-packages\\paddleocr\\_common_args.py:43\u001b[39m, in \u001b[36mparse_common_args\u001b[39m\u001b[34m(kwargs, default_enable_hpi)\u001b[39m\n\u001b[32m     41\u001b[39m unknown_names = kwargs.keys() - default_vals.keys()\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m unknown_names:\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnknown argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     45\u001b[39m kwargs = {**default_vals, **kwargs}\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs[\u001b[33m\"\u001b[39m\u001b[33mprecision\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m SUPPORTED_PRECISION_LIST:\n",
      "\u001b[31mValueError\u001b[39m: Unknown argument: use_gpu"
     ]
    }
   ],
   "source": [
    "\n",
    "def run_pipeline():\n",
    "    img0 = ensure_img(INPUT_PATH)\n",
    "    img  = align_to_a4(img0, A4_SIZE_PX)[0] if ALIGN_TO_A4 else img0\n",
    "\n",
    "    H, W = img.shape[:2]\n",
    "    polys = run_dbnet_detector(img)\n",
    "\n",
    "    words = []\n",
    "    crops_dir = OUTPUT_DIR / \"crops\"\n",
    "    overlay   = img.copy()\n",
    "\n",
    "    idx = 1\n",
    "    for poly in polys:\n",
    "        x0,y0,x1,y1 = poly_to_bbox(poly)\n",
    "        if (x1-x0)*(y1-y0) < MIN_AREA_PX:\n",
    "            continue\n",
    "        cx = (x0+x1)/2.0; cy = (y0+y1)/2.0\n",
    "        cv2.polylines(overlay, [poly.astype(np.int32)], True, (0,255,0), 2)\n",
    "        cv2.rectangle(overlay, (x0,y0), (x1,y1), (255,0,0), 1)\n",
    "        fname = f\"word_{idx:04d}.png\"\n",
    "        save_crop(img, (x0,y0,x1,y1), crops_dir / fname)\n",
    "        words.append({\n",
    "            \"id\": idx,\n",
    "            \"poly\": poly.round(2).tolist(),\n",
    "            \"bbox_xyxy\": [int(x0),int(y0),int(x1),int(y1)],\n",
    "            \"center\": [float(cx), float(cy)],\n",
    "            \"size\": [int(x1-x0), int(y1-y0)],\n",
    "            \"crop_path\": str((crops_dir / fname).as_posix())\n",
    "        })\n",
    "        idx += 1\n",
    "\n",
    "    # Border estimation\n",
    "    if BORDER_MODE == \"fixed\":\n",
    "        x_border = int(FIXED_BORDER_RATIO * W)\n",
    "    else:\n",
    "        LEFT_CAND_MAX_X = LEFT_CAND_MAX_XR * W\n",
    "        cand = []\n",
    "        for w in words:\n",
    "            cx, cy = w[\"center\"]\n",
    "            bw, bh = w[\"size\"]\n",
    "            if cx <= LEFT_CAND_MAX_X and bw <= 0.18*W and bh >= 0.012*H:\n",
    "                cand.append(w)\n",
    "        if not cand:\n",
    "            x_border = int(0.30 * W)\n",
    "        else:\n",
    "            xs     = np.array([w[\"center\"][0] for w in cand], dtype=np.float32)\n",
    "            widths = np.array([w[\"size\"][0]   for w in cand], dtype=np.float32)\n",
    "            x_med  = float(np.median(xs))\n",
    "            w_med  = float(np.median(widths) if len(widths) else 20.0)\n",
    "            x_border = int(x_med + max(LEFT_PAD_PX, 0.5*w_med))\n",
    "\n",
    "    # Mark anchors & border\n",
    "    anchors = []\n",
    "    for w in words:\n",
    "        cx, cy = w[\"center\"]\n",
    "        if cx <= x_border:\n",
    "            anchors.append(w)\n",
    "            x0,y0,x1,y1 = w[\"bbox_xyxy\"]\n",
    "            cv2.rectangle(overlay, (x0,y0), (x1,y1), (0,0,255), 2)\n",
    "    anchors.sort(key=lambda w: w[\"center\"][1])\n",
    "    cv2.line(overlay, (x_border, 0), (x_border, H-1), (0,165,255), 2)\n",
    "    save_image(overlay, \"debug_overlay.png\")\n",
    "\n",
    "    # Group right-side words\n",
    "    groups = []\n",
    "    if anchors:\n",
    "        edges_y = [0] + [int(a[\"center\"][1]) for a in anchors] + [H-1]\n",
    "        bounds = []\n",
    "        for i in range(1, len(edges_y)-1):\n",
    "            y_mid_prev = int((edges_y[i-1] + edges_y[i]) / 2)\n",
    "            y_mid_next = int((edges_y[i]   + edges_y[i+1]) / 2)\n",
    "            y0 = max(0, y_mid_prev + TOP_PAD)\n",
    "            y1 = min(H-1, y_mid_next - BOTTOM_PAD)\n",
    "            bounds.append((i, y0, y1))\n",
    "\n",
    "        for idx, y0, y1 in bounds:\n",
    "            grp_dir = OUTPUT_DIR / \"groups\" / f\"Q{idx:03d}\"\n",
    "            grp_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            anc = anchors[idx-1]\n",
    "            ax0,ay0,ax1,ay1 = anc[\"bbox_xyxy\"]\n",
    "            save_crop(img, (ax0,ay0,ax1,ay1), grp_dir / \"anchor.png\")\n",
    "\n",
    "            pack = {\"group\": f\"Q{idx:03d}\", \"y_range\": [y0,y1], \"anchor_id\": anc[\"id\"], \"words\": []}\n",
    "            for w in words:\n",
    "                cx, cy = w[\"center\"]\n",
    "                if cx > x_border and y0 <= cy <= y1:\n",
    "                    src = Path(w[\"crop_path\"])\n",
    "                    dst = grp_dir / src.name\n",
    "                    im  = cv2.imread(str(src))\n",
    "                    if im is not None:\n",
    "                        cv2.imwrite(str(dst), im)\n",
    "                    pack[\"words\"].append({**w, \"group_path\": str(dst.as_posix())})\n",
    "            groups.append(pack)\n",
    "    else:\n",
    "        grp_dir = OUTPUT_DIR / \"groups\" / \"Q001\"\n",
    "        grp_dir.mkdir(parents=True, exist_ok=True)\n",
    "        pack = {\"group\": \"Q001\", \"y_range\": [0,H-1], \"anchor_id\": None, \"words\": []}\n",
    "        for w in words:\n",
    "            cx, cy = w[\"center\"]\n",
    "            if cx > x_border:\n",
    "                src = Path(w[\"crop_path\"])\n",
    "                dst = grp_dir / src.name\n",
    "                im  = cv2.imread(str(src))\n",
    "                if im is not None:\n",
    "                    cv2.imwrite(str(dst), im)\n",
    "                pack[\"words\"].append({**w, \"group_path\": str(dst.as_posix())})\n",
    "        groups.append(pack)\n",
    "\n",
    "    save_json({\"border_x\": x_border, \"width\": W, \"height\": H, \"words\": words}, \"words.json\")\n",
    "    save_json({\"groups\": groups}, \"groups_index.json\")\n",
    "\n",
    "    print(f\"Border x = {x_border} (W={W}) | anchors={len(anchors)} | total words={len(words)}\")\n",
    "    for g in groups:\n",
    "        print(f\"{g['group']}: {len(g['words'])} right-side words; y_range={g['y_range']}\")\n",
    "\n",
    "run_pipeline()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1cb365",
   "metadata": {},
   "source": [
    "## Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593fd05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pprint import pprint\n",
    "files = sorted([p.as_posix() for p in OUTPUT_DIR.rglob(\"*\") if p.is_file()])\n",
    "pprint(files[:60])\n",
    "print(\"... total files:\", len(files))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
